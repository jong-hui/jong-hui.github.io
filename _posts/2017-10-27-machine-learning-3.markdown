---
layout: post
title:  "[머신러닝 배우기] 3.머신러닝의 주요 개념 - 모델"
subtitle:   "ML"
categories: devlog
tags: ml
comments: true
---

## 머신러닝의 주요 개념

머신러닝에 대해 잘 알아야 좋은 데이터를 기계에게 공급할 수 있다. 머신러닝 시스템을 차에, 데이터를 연로에 비유할 수 있다. 경유가 필요한 차에 휘발유를 넣거나, 기름이 많이 필요한 차에 기름을 적게 주거나 하는 일을 줄이기 위해 우리는 이론을 더 잘 알아야 한다.

머신러닝은 크게 4가지의 핵심 개념을 갖는다. 

## 모델

> 모델 : 문제를 바라보는 관점

모델은 쉽게 말해 가정이라고 할 수 있다. `데이터가 이런 패턴을 가지지 않을까?`라는 가정을 머신러닝에서 모델이라고 하는 것이다. 혹자는 믿음이라고 표현하기도 한다.

사실 가정, 믿음, 모델, 기대... 등등 모두 같은 표현이라고 할 수 있다. 머신러닝을 한다는 건 일반적으로 아래와 같은 형식을 갖춘다.

```
모델 정하기 -> 모델 수식화하기 -> 모델 학습하기 -> 모델 평가하기
```

즉, 모델은 머신러닝의 핵심이라고 할 수 있다. 모델은 크게 간단한 모델, 복잡한 모델, 구조화 된 모델로 볼 수 있다.

### 간단한 모델

모델이 간단하다는 건 데이터 구조가 간단하다는 이야기와 같다. 모델은 가정이라고 이야기 했는데 쉽게 이야기하면 간단한 모델은 `아주 강력한 가정`을 했다는 이야기와 같다.

간단한 모델은 대부분 선형모델을 이야기하며, 선형 모델은 간단한 만큼 이해하기가 쉽다. 예를들어 공장의 생산량과 불량품 수에 대한 관계는 일반적으로 선형모델을 사용하여 풀 수 있다.

간단한 모델은 쉬운 반면에 복잡한 관계는 학습 할 수가 없다. 단순한 문제라고 생각하였는데 실제론 복잡한 문제였다면 엉뚱한 답이 나오게 되는 것이다.

### 복잡한 모델

데이터가 복잡하다고 가정하는 경우가 복잡한 모델이다. 데이터 과학이라면서 간단하고 복잡하다는 상대적인 이야기를 하고있어? 라고 생각할 수 있지만, 이해하기 쉽게 하기 위한 표현이지 사실 모델의 간단하고 복잡하고의 관점은 유연성에 있다.

즉, 복잡한 모델은 더 유연한 모델이다. 복잡한 모델의 대표적인 예는 결정트리인데, 결정트리는 유연성이 뛰어나 많은 종류의 데이터를 모델링할 수 있다. 하지만 예외사항까지 일일이 가정을 만들기 때문에 노이즈가 생겨서 학습에 어려움을 초래하기 쉽다.

### 구조가 있는 모델

구조가 있는 모델은 앞선 모델들과 다르게 입력과 출력의 관계만을 학습하는게 아니라 데이터 구조 자체를 모델링하는 모델이다. 구조가 있는 모델은 간단한 모델이 될 수도있고, 복잡한 모델이 될 수도있다. 대표적인 모델로는 순차모델과 그래프모델이 있다. 

순차모델은 연속된 관측값이 서로 연관성이 있을 때 주로 사용한다. 텍스트, 시간과 관련된 데이터 분석이 대표적이다. 특정 시점에 상태를 저장하고, 상태가 각 시점의 입력과 출력에 따라 변화한다는 점이 가장 큰 특징이다. 쉽게 이야기해서 상태라는 중간데이터를 만든다고 생각하면 되는데, 이 상태가 다음 상태에 영향을 주는 형식이다. 상태를 가지지 않는 모델도 현 시점의 입력과 이전시점의 입력을 조합해 비슷한 효과를 낼 수 있지만, 모든 시점의 입력을 이용하기에는 계산의 한계가 있어 넓은 범위의 의존성을 나타내기 어렵다.

그래프모델은 순차모델보다 조금 더 복잡한 구조를 모델링할 때 사용한다. 문서의 문법구조를 모델링하거나 이미지의 픽셀간의 관계를 모델링하거나 하는 식이다.

## 좋은 모델이란?

좋은 모델이란 `데이터의 패턴을 잘 학습한 모델`이라고 할 수 있다. 어떤 데이터는 간단한 모델로 학습할 때가 적합하고, 어떤 모델은 복잡한 모델로 학습하는 때가 더 효과가 좋다. 절대진리의 모델은 없으며, 상황에 따라, 문제에 따라 좋은 모델을 찾는것이 머신러닝 엔지니어의 자질이라고 할 수 있다.

하지만, 아무리 뛰어난 머신러닝 엔지니어라도 미세한 차이 또는 애매한 차이를 판가름 하여 어떤 모델이 좋은모델인지 확신하기는 어렵다. 그 어떤 모델이 더 좋은 모델인지를 평가하는 것을 돕기위해 모델 평가이론이 등장하였다. 가장 널리 사용되는 모델 평가이론은 모델의 복잡도와 표현력에 대한 균형을 다루는 `편향-분산 트레이드오프`, 균형을 자동으로 학습하는 `정규화`가 있다.

### 편향-분산 트레이드오프

편향-분산 트레이드오프의 수식은 아래와 같다. 잠깐 수식 이야기를 하게 되서 머리가 지끈거릴 수 있지만 특징만 설명하고 간단히 넘어갈 테니 쓱 한번 보자.

![편향-분산 트레이드오프](https://wikimedia.org/api/rest_v1/media/math/render/svg/328075a763fe06d0201748100d57b53bd85a89db)

이 식을 살펴보면 모델이 데이터를 예측할 때 생기는 오류는 편향의 제곱과 분산으로 표현될 수 있다는 것을 알 수 있다. 즉, 오류를 줄여 모델이 더 좋은 성능을 내려면 편향을 줄이거나 분산을 줄여야 한다는 이야기가 된다.

여기서 편향은 이렇게 생겼다.

![편향](https://wikimedia.org/api/rest_v1/media/math/render/svg/5acc7f11d38f737d9c32d537eb0006945e5dc95e)

편향은 쉽게 이야기해 데이터로 학습한 결과와 이상적인 모델 간의 차이를 말한다. 앞서 설명한 간단한 모델같은 경우는 표현력이 좋지 않기 때문에 이상적인 데이터와는 사실 차이가 꽤 난다. 즉 간단한 모델은 편향이 큰 편이다. 물론 유연해서 다양한 표현이 가능한 복잡한 모델도 데이터와 잘 맞지 않으면 편향이 크게 나올 수 있다.

분산은 아래와 같이 생겼다.

![분산](https://wikimedia.org/api/rest_v1/media/math/render/svg/c938173fc2d2ac97408bdec7d318b3594285a76a)

분산은 데이터를 이용해서 얻은 모델이 학습할 때마다 얼마나 달라질 수 있는지를 나타낸다. 보통 모델이 복잡할수록 편차가 크기 때문에 분산이 더 크게 나타나는 경향이 있다.

쉽게 이야기해서 편향과 분산을 모두 줄여야 하는데, 모델이 복잡하면 편향은 줄어드는 경향이 있지만 분산이 높아지는 경향을 보이는 문제가 있다. 반대로 간단한 모델은 분산이 작아지지만 편향이 커지곤한다. 이런 상황속에서 우리는 데이터에 걸맞는 적절한 복잡도의 모델을 설계해야 한다.

이를 해결하기 위한 기법으로 더 좋은 결과를 얻기 위해 부스팅과 랜덤포레스트라는 기법을 사용하곤한다. 부스팅은 간단한 모델을 여러개 조합하여 편향을 줄이는 방법이고, 랜덤포레스트는 복잡한 모델을 여러개 조합하여 분산을 줄이는 방법이다.

### 정규화

정규화는 정해진 모델이 필요 이상으로 복잡해지지 않도록 조절하는 트릭이다. 모델이 데이터에 비해 복잡하면 불필요한 노이즈까지 학습해서 학습시에는 좋아보일 수 있지만, 실사용에는 좋은 성능을 발휘하지 못할 수 있다. 이 때 모델의 복잡도를 조절하기 위해 엔지니어는 모델자체를 변경할 것인지, 정규화를 할 것인지의 갈림길에 서게 된다.

어떤 식이 있을 때 새로운 제약을 거는 것이다. 인자가 3개일 때 `인자 중 하나는 0이다.`라는 제약을 건다면 식이 단순해진다. 이런경우 3가지중 하나를 제외한 2가지를 고르는 경우의 수인 3가지 식이 나오게 되는데, 이 3가지를 비교해서 더 좋은 결과를 찾아 사용하면 된다. 이런식으로 정규화는 실제로도 굉장히 자주 사용되며, 꽤나 좋은 성능을 낸다.

## 손실함수

> 모델의 정확도

모델이 실제로 데이터를 바르게 표현했는지, 얼마나 예측이 정확한지를 수학적인 표현으로 알 수 있는 것이 손실함수이다. 사실 오차의 정도를 표현하는 식인데, 결국 오차의 정도가 작으면 정확한 식을 찾았다는 것이므로 정확도를 측정하는 식으로 알려져있다.



