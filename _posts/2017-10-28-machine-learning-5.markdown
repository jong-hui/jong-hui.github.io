---
layout: post
title:  "[머신러닝 배우기] 5.머신러닝의 주요 개념 - 최적화"
subtitle:   "ML"
categories: devlog
tags: ml
comments: true
---

## 최적화

> 실제로 학습을 하는 방법

최적화는 손실함수를 이용해서 모델을 학습하는 방법이라고 할 수 있다. 손실함수의 결괏값을 최소화하는 모델의 인자를 찾는 것이 최적화인 것이다.

인자에 임의의 값을 넣어보면서 해볼 수 있지만, 보통은 수학적으로 최적의 값을 찾도록 한다. 쉽게 이야기해서 고등학교때 배웠던 함수의 최솟값, 최댓값을 찾는 것과 같다.

### 경사하강법

경사하강법은 쉽게이야기해서 U 형태의 포물선함수가 있을때 임의의 점에서 시작해 경사를 타고 내려가면서 계산하는 방법이다. 여러번 진행하면 가장 아래부분의 평평한 부분으로 내려가게 된다.

경사를 구하는건 미분을 하면 구할 수 있기 때문에 한번에 몇칸씩 움직일지만 설정하면 쉽게 구현할 수 있는 최적화 방식이다. 한번에 움직이는 크기(학습률)가 크면 빠르게 학습할 수 있지만 너무 크면 최솟값에 수렴하지 않고 계속 최솟값 근처를 왔다갔다하며 머물수 있기때문에 경사의 크기가 작아질수록 한번에 움직이는 크기를 줄이는 방식을 사용하기도 한다.

### 뉴턴/준뉴턴 방법

뉴턴방식은 1차미분과 2차미분을 사용한 방식인데, 미분과 풀이에 너무 많은 자원이 들어가 잘 사용하지 않는다. 그래서 1차미분으로 2차미분을 유추하는 방법을 사용하는데 그 방식이 준뉴턴방식이다. 그럼에도 불구하고 자원이 많이 들기때문에 데이터가 적을때에 주로 사용한다.

### 확률적 경사하강법

확률적 경사하강법은 몇개의 샘플을 뽑아 손실함수와 1차미분값을 사용한다. 즉 뉴턴/준뉴턴이나 다른 방식들보다 자원이 훨씬 적게 들기때문에 엄청 많은 데이터를 다룰 때 주로 사용된다.

### 역전파

최근 딥러닝이 뜨면서 가장 많이 사용되는 방식이다. 딥러닝은 손실함수도 층층이 쌓인 구조가 되는데 그래서 중간중간 함수들이 많은 부분을 차지한다. 방식은 조금 복잡하지만 출력에서부터 입력으로 손실을 올려보낸다고 생각하면 쉽다.

### Adam

학습률에 따라 성능이 좌지우지 되는 경사하강법이나 확률적 경사하강법을 위해 많은 연구들이 진행되었다. 그 결과 최근에는 현재의 1차미분값과 과거의 미분값의 차이를 보면서 학습률을 자동으로 조절하는 방식이 많이 사용된다. 그 중 가장 유명한 방식이 Adam이다.

Adam은 과거의 미분값의 방향과 분산을 계속 가중평균을 내면서 업데이트 방향과 크기를 책정한다.

위의 모든 방법들은 직접구현할 필요는 없다고 보면 된다. 이미 구현된 라이브러리를 잘 사용하기만 해도 되기때문에 개념을 잘 이해하고 적절한 곳에 이용하는 것이 중요하다.

## 모델평가

> 실제 활용에서 성능을 평가하는 방법

모델이 얼마나 좋은 성능을 보일지 평가하는 방법을 말한다. 손실함수와 완전히 같은경우도 있지만 개념이 서로 상이하다.

모델 평가를 할 때는 학습 데이터뿐만아니라 새로운데이터가 들어왔을때 잘 동작하는지를 측정하는데, 이를 일반화라고한다. 일반화란 실제 머신러닝 시스템을 구축할 때 굉장히 중요한 요소인데, 학습데이터는 편향되어 있을 수가 있기 때문이다.

모델의 복잡도를 조절하기위해 정규화를 배웠는데, 모델 평가에서는 정규화를 얼마나 강하게 해야할지와 모델이 관측되지 않은 데이터에 대해 잘 동작할지를 평가한다. 크게 특성, 정확도, 정밀도를 평가해 사용한다.









